---
title: "Letter_Identification"
author: "Yitong Chen"
date: "10/21/2020"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE,warning=FALSE}
library(ggplot2)
library(caTools)
library(plotROC)
library(ROCR)
library(rpart) # CART
library(rpart.plot) # CART plotting
library(caret) # c
library(randomForest)
library(gbm)
library(GGally)
library(MASS)
```


## 1.1 Simple Intro on is "B"
To make the problem easier, as well as getting to know the big picture of the data, we'll start by predicting whether or Not the letter is "B"  
```{r}
## load the data
Letters = read.csv("Letters.csv",sep = ",")
```

#### add an "isB" column on the dataset 
```{r}
Letters$isB = as.factor((Letters$letter == "B"))
```

#### Train Test Split
Put 65% of the data in the training set and 35% in the test set
```{r}
train.ids = sample(nrow(Letters), 0.65* nrow(Letters))
Letters.train = Letters[train.ids, ]
Letters.test = Letters[-train.ids, ]

```

#### 1 i) Baseline Model
This model always predicts "not B", we'll derive its performance in test set
```{r}
table(Letters.train$isB)
Accuracy = sum(Letters.train$isB == FALSE) / nrow(Letters.train)
table(Letters.test$isB)
Base_Accuracy = sum(Letters.test$isB == FALSE) / nrow(Letters.test)

paste("The Accuracy of the baseline model:", Base_Accuracy) 
```

#### 1 ii) Logistic Regression
A typical logistic regression that include all the variables, threshold is set at $ p = 0.5$
```{r}
## remove the letter for training
log_model = glm(data = subset(Letters.train, select=c(-letter) ), family = "binomial", isB ~.)
summary(log_model)

## predict on the test set
pisB <- predict(log_model, newdata = subset(Letters.test, select=c(-letter) ), type = "response")
pisB = pisB > 0.5

## construct the confusion matrix
confusion_m <- table(Letters.test$isB, pisB)
confusion_m
log_accuracy = (confusion_m[1] + confusion_m[4])/sum(confusion_m)
paste("The Accuracy of the logistic regression model:", log_accuracy)
```

We also calculate the AUC of the model
```{r}
predTestLog <- predict(log_model, newdata = subset(Letters.test, select=c(-letter) ), type = "response")
rocr.log.pred <- prediction(predTestLog, Letters.test$isB)
logPerformance <- performance(rocr.log.pred, "tpr", "fpr")
plot(logPerformance, colorize = TRUE)
abline(0, 1)

# AUC
paste("As calulated, AUC = ",as.numeric(performance(rocr.log.pred, "auc")@y.values))
```


#### 1 iii) CART Model to predict "isB"
Cross Validation is applied to select the cp(complexity parameter) of the model. 
```{r}
## A list of cp value will be test 
cpVals = data.frame(cp = seq(0, .1, by=.001)) 

train.cart <- train(isB ~ .,
                    data = subset(Letters.train, select=c(-letter) ),
                    method = "rpart",
                    tuneGrid = cpVals,
                    trControl = trainControl(method = "cv", number=10),
                    metric = "Accuracy")

Letters.test.mm = as.data.frame(model.matrix(isB~.+0, data=Letters.test))

## a threshold of 0.5 is set for decision
pred.cart <- predict(train.cart,newdata = Letters.test.mm, type="prob" )
pred_cart = pred.cart[,2] > 0.5

confusion_m <- table(Letters.test$isB, pred_cart)
confusion_m

cart_accuracy = (confusion_m[1] + confusion_m[4])/sum(confusion_m)
paste("The Accuracy of the CART model:", cart_accuracy)
```

```{r}
# A cp vs Accuracy to help us on deciding cp
ggplot(train.cart$results, aes(x=cp, y=Accuracy)) + geom_point(size=3) +
  xlab("Complexity Parameter (cp)") + geom_line()
```



#### 1 iv) Random Forest to predict "isB"
```{r}
## train the RF model
mod.rf <- randomForest(isB ~ .,
                    data = subset(Letters.train, select=c(-letter)))

pred.rf <- predict(mod.rf, newdata = Letters.test, type = "response") 

confusion_m <- table(Letters.test$isB, pred.rf)
confusion_m
rf_accuracy = (confusion_m[1] + confusion_m[4])/sum(confusion_m)
paste("The Accuracy of the Random Forest :", rf_accuracy)
```

#### 1 v) Summary
```{r}
data.frame(Model = c("Baseline","Logistic Regression","CART","Random Forest"), Accuracy = c(Base_Accuracy, log_accuracy, cart_accuracy, rf_accuracy))
```

## 2. Prediction on "A", "B", "P", "R"
Now, let's move on to a more general case: predicting a letter is one of "A", "B", "P", "R". It can be extended to the full 26 letters case, but this will inflate the model complexity and run time. For the sake of simplicity, we'll run the demo of four letter case here.

#### 2 i) Baseline Model
```{r}
table(Letters.train$letter)

table(Letters.test$letter)

## Since "A" appears most frequently in the training set, we'll always predict the "A" in the test set
Base_Accuracy = sum(Letters.test$letter == "A") / nrow(Letters.test)
paste("Baseline Model Accuracy = " ,Base_Accuracy) 
```


#### 2 ii) LDA (Latent Dirichlet allocation)
```{r}
LdaModel <- lda(letter ~ ., data=subset(Letters.train, select = c(-isB)))

predTestLDA <- predict(LdaModel, newdata = Letters.test)

confusion_m <- table(predTestLDA$class, Letters.test$letter)
confusion_m

lda_accuracy = sum(predTestLDA$class == Letters.test$letter)/nrow(Letters.test)
paste("LDA Accuracy:" ,lda_accuracy)
```

#### 2 iii) CART
```{r}
cpVals = data.frame(cp = seq(0, .001, by=.1))

train.cart_2 <- train(letter ~ .,
                    data = subset(Letters.train, select=c(-isB) ),
                    method = "rpart",
                    tuneGrid = cpVals,
                    trControl = trainControl(method = "cv", number=5),
                    metric = "Accuracy")

train.cart_2


Letters.test.mm = as.data.frame(model.matrix(letter~.+0, data=Letters.test))

pred.cart <- predict(train.cart_2, newdata = Letters.test.mm, type="prob" )
pred.cart$results = colnames(pred.cart)[apply(pred.cart,1,which.max)]

confusion_m <- table(Letters.test$letter, pred.cart$results)
confusion_m

cart_accuracy_2 = sum(Letters.test$letter == pred.cart$results)/(nrow(Letters.test))
paste("CART Accuracy" ,cart_accuracy_2)
```


```{r}
tree <- rpart(letter ~., data = Letters.train, method = "class")
rpart.plot(tree)
```

#### 2 iv) Bagging of CART models
```{r}
## fit the model
train.letter.mm = as.data.frame(model.matrix(letter ~ . + 0, data = subset(Letters.train, select=c(-isB))))

mod.bag <- randomForest(x = train.letter.mm, y = Letters.train$letter, mtry = 16)
mod.bag
```


```{r}
## display the result 
Letters.test.mm = as.data.frame(model.matrix(letter~.+0, data=Letters.test))

pred.bag <- predict(mod.bag, newdata = Letters.test.mm)

confusion_m <- table(pred.bag, Letters.test$letter)
confusion_m 

bagging_accuract = sum(pred.bag == Letters.test$letter)/nrow(Letters.test)
paste("Bagging of CART Accuracy: ", bagging_accuract)
```

#### 2 v) Random Forest 
We use 5-fold cross validation to find the best entry of mtry
```{r}
mod.rf <- randomForest(letter ~ ., data = subset(Letters.train, select=c(-isB) ), mtry = 5)
mod.rf

pred.rf <- predict(mod.rf, newdata = Letters.test) 
table(pred.rf, Letters.test$letter)
rf_acc = sum(pred.rf == Letters.test$letter)/nrow(Letters.test)
rf_acc
```

#### 2 vi) Gradient Boosting 
```{r, message=FALSE}
## fit the gradient boosting machine 
mod.boost <- gbm(letter ~ .-isB + 0, 
                 data = Letters.train,
                 distribution = "multinomial",
                 n.trees = 3300,
                 interaction.depth = 10)

mod.boost
```


```{r}
pred.boost <- predict(mod.boost, newdata = subset(Letters.test, select=c(-isB)), type = "response",n.trees = 3300) 
pred.boost <- apply(pred.boost, 1, which.max)
pred.boost <- factor(pred.boost,levels = c(1,2,3,4),labels = c("A","B","P","R"))

confusion_m <- table(pred.boost,Letters.test$letter)
confusion_m

gbm_acc = sum(pred.boost == Letters.test$letter)/nrow(Letters.test)

paste("Gradient Boosting Accuracy: ", gbm_acc)
```


#### 2 vii) Summary on the methods we used for recognizing "A" "B" "P" "R
```{r}
data.frame(Model = c("Baseline","CART","CART Bagging","Random Forest","Gradient Boosting"), Accuracy = c(Base_Accuracy, cart_accuracy_2, bagging_accuract, rf_acc, gbm_acc))
```

A short summary to all the methods above, the Gradient boosting is giving the best performance comparing to others. Without other assumptions, GBM will be the model selected for further analysis on letter recognition. Random forest also have a good performance among other models, and a good thing of it is the time cost on training is much lower than boosting, whereas its test set accuracy is only 0.0027 lower.





